Arguments:
	       active_rate : 0.1
	  aggregation_rate : 0.4
	     attack_method : 0
	      attack_range : 0.3
	       attack_rate : 0.1
	        batch_size : 10
	 clients_per_round : 10
	    committee_rate : 0.4
	           dataset : sent140
	      drop_percent : 0.0
	        eval_every : 1
	     learning_rate : 0.01
	      merge_method : 0
	             model : stacked_lstm
	      model_params : (25, 2, 100)
	                mu : 0
	        num_epochs : 20
	         num_iters : 1
	        num_rounds : 200
	         optimizer : fedavg
	              seed : 0
	selection_strategy : 1
Using Federated avg to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/726.88k flops)
  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Initializer/random_uniform (160.00k/320.00k flops)
    rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Initializer/random_uniform/mul (160.00k/160.00k flops)
    rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Initializer/random_uniform/sub (1/1 flops)
  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Initializer/random_uniform (80.00k/160.00k flops)
    rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Initializer/random_uniform/mul (80.00k/80.00k flops)
    rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/Add (160.00k/160.00k flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/Add (80.00k/80.00k flops)
  dense/kernel/Initializer/random_uniform (3.00k/6.00k flops)
    dense/kernel/Initializer/random_uniform/mul (3.00k/3.00k flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/Add (400/400 flops)
  gradients/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/Add (400/400 flops)
  dense_1/kernel/Initializer/random_uniform (30/61 flops)
    dense_1/kernel/Initializer/random_uniform/mul (30/30 flops)
    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  rnn/while/add (1/1 flops)
  sigmoid_cross_entropy_loss/num_present/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/is_same_rank (1/1 flops)
  sigmoid_cross_entropy_loss/num_present/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims (1/1 flops)
  sigmoid_cross_entropy_loss/num_present/broadcast_weights/assert_broadcastable/is_scalar (1/1 flops)
  sigmoid_cross_entropy_loss/num_present/Equal (1/1 flops)
  sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/is_same_rank (1/1 flops)
  sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims (1/1 flops)
  sigmoid_cross_entropy_loss/assert_broadcastable/is_scalar (1/1 flops)
  rnn/while/add_1 (1/1 flops)
  gradients/Sub (1/1 flops)
  rnn/while/Less_1 (1/1 flops)
  rnn/while/Less (1/1 flops)
  gradients/Add (1/1 flops)
  gradients/GreaterEqual (1/1 flops)
  rnn/Minimum (1/1 flops)
  rnn/Maximum (1/1 flops)
  gradients/sigmoid_cross_entropy_loss/value_grad/mul (1/1 flops)
  gradients/sigmoid_cross_entropy_loss/value_grad/Neg (1/1 flops)

======================End of Report==========================
772 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.520980669495521
At round 0 training accuracy: 0.5209139601845258
At round 0 training loss: 0.6892470620952489
At round 1 accuracy: 0.6212871287128713
At round 1 training accuracy: 0.6210099383881854
At round 1 training loss: 0.6665104460886394
At round 2 accuracy: 0.6212871287128713
At round 2 training accuracy: 0.6210099383881854
At round 2 training loss: 0.6755833291434264
At round 3 accuracy: 0.6168081093823669
At round 3 training accuracy: 0.6090900647078857
At round 3 training loss: 0.6714110311356384
